services:

  #################### ZOOKEEPER ####################
  zookeeper:
    container_name: zookeeper
    image: confluentinc/cp-zookeeper:7.7.7

    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

    ports:
      - "2181:2181"

    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
    
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 10s
      timeout: 5s
      retries: 5

    networks:
      - local


  #################### KAFKA ####################
  kafka:
    container_name: kafka
    image: confluentinc/cp-kafka:7.7.7

    depends_on:
      - zookeeper

    environment:
      # learn about this variables.
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # 9092 for internal docker comms and 29092 for host machine comms
      # I guess changing this might corrupt the metadata of kafka.
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

    ports:
      - "9092:9092"
      - "29092:29092"

    volumes:
      - kafka-data:/var/lib/kafka/data

    networks:
      - local


  #################### KAFKA UI ####################
  # If it stuck in loop then delete the volume kafka-data stored in host machine and try again 
  # Use following commands
  # docker compose down -v
  # docker compose up -d
  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest

    ports:
      - "8080:8080"

    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    # KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181

    depends_on:
      - kafka

    networks:
      - local


  #################### MONGODB ####################
  mongodb:
    container_name: mongodb
    image: mongo:6.0-rc-jammy

    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password

    ports:
      - "27017:27017"

    volumes:
      - mongodb-data:/data/db

    networks:
      - local


  #################### ELASTICSEARCH ####################
  elastic-search:
    container_name: elastic-search
    image: elasticsearch:9.2.2

    environment:
      discovery.type: single-node
      xpack.security.enabled: "false"
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"

    ports:
      - "9200:9200"
      - "9300:9300"

    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data

    networks:
      - local


  #################### KIBANA ####################
  kibana:
    container_name: kibana
    image: kibana:9.2.2

    depends_on:
      - elastic-search

    environment:
      ELASTICSEARCH_HOSTS: http://elastic-search:9200

    ports:
      - "5601:5601"

    networks:
      - local

  # #################### Spark ####################
  spark-stream-master:
    build:
      context: .
      dockerfile: ./spark/Dockerfile
    container_name: spark-stream-master
    environment:
      - SPARK_MODE=master
      - WEBUI_PORT=2020
      - MASTER_PORT=3033
      - PYTHONPATH=/opt/spark-apps
    ports:
      - "2020:2020"  # Spark Master UI
      - "1011:1011"  # RPC
    volumes:
    - spark_warehouse:/opt/spark-apps/spark-warehouse
    - ./src/apps:/opt/spark-apps
    - ./src:/opt/spark-apps/src
    - spark_data:/opt/spark-data
    - ./spark/entrypoint.sh:/opt/entrypoint.sh
    networks:
      - local
    entrypoint: ["/bin/bash","/opt/entrypoint.sh"]

  spark-stream-worker-1:
    build:
      context: .
      dockerfile: ./spark/Dockerfile
    container_name: spark-stream-worker-1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-stream-master:3033
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - WORKER_PORT=2021
      - MASTER_PORT=3033
      - PYTHONPATH=/opt/spark-apps
    depends_on:
      - spark-stream-master
    ports:
      - "2021:2021"
    volumes:
      - spark_warehouse:/opt/spark-apps/spark-warehouse
      - spark_data:/opt/spark-data
      - ./src:/opt/spark-apps/src
      - ./spark/entrypoint.sh:/opt/entrypoint.sh
    networks:
      - local
    entrypoint: ["/bin/bash","/opt/entrypoint.sh"]

# #################### PostgreSQL ####################
# postgres:
#   image: postgres:16-alpine
#   container_name: postgres
#   env_file:
#     - .env
#   # NOTE: POSTGRES_USER, POSTGRES_PASSWORD, and POSTGRES_DB should be in your .env file
#   environment:
#     - POSTGRES_HOST_AUTH_METHOD=password
#   ports:
#     - "5432:5432"
#   volumes:
#     - postgres_data:/var/lib/postgresql/data
#   healthcheck:
#     test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
#     interval: 5s
#     timeout: 5s
#     retries: 10
#     start_period: 30s
#   restart: unless-stopped
#   networks:
#     - local

# #################### Airflow ####################
# airflow:
#   build:
#     context: .
#     dockerfile: ./airflow/Dockerfile
#   container_name: airflow-w-pg
#   depends_on:
#     postgres:
#       condition: service_healthy
#   env_file:
#     - .env
#   environment:
#     - PYTHONPATH=/opt/airflow
#   volumes:
#     - ./airflow/dags:/opt/airflow/dags
#     - airflow_logs:/opt/airflow/logs
#     - ./airflow/plugins:/opt/airflow/plugins
#     - ./airflow/src:/opt/airflow/src
#   ports:
#     - "9099:9099" # Port for Airflow Webserver
#     - "8793:8793" # Port for Airflow Celery Flower
#   healthcheck:
#     test: ["CMD", "curl", "-f", "http://localhost:9099/health"]
#     interval: 30s
#     timeout: 10s
#     retries: 3
#     start_period: 120s
#   networks:
#     - local



#################### VOLUMES ####################
volumes:
  zookeeper-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/zookeeper_data
  kafka-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/kafka_data
  mongodb-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/mongodb_data
  elasticsearch-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/elasticsearch_data
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/pg_data
  airflow_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/airflow_logs
  spark_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/spark_data
  spark_warehouse:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: F:/docker-images/volumes/spark_warehouse

#################### NETWORK ####################
networks:
  local:
    driver: bridge